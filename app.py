#!/usr/bin/env python3
"""
LLM.txt Generator Web Application
---------------------------------
A comprehensive web application that creates LLM.txt files for each page of a given website.

Features:
- Professional UI/UX with smooth animations
- Real-time logging and progress tracking
- Comprehensive error handling
- Database storage for results
- Integration with sitemap, scraper, and screenshot APIs
- ChatGPT integration for LLM.txt generation

Author: AEO-Maker
Date: 2025-01-04
"""

import os
import sys
import json
import sqlite3
import threading
import time
import uuid
import requests
import openai
import subprocess
import platform
import signal
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

from flask import Flask, render_template, request, jsonify, send_file
from flask_socketio import SocketIO, emit

# Add service directories to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'Sitemap_Service'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'Text_Scrapper_Service'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'Screenshot_Service'))

# Initialize Flask app with SocketIO for real-time updates
app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-change-this'
socketio = SocketIO(app, cors_allowed_origins="*")

# Database configuration
DATABASE_PATH = 'llm_generator.db'

# API endpoints configuration
SITEMAP_API_URL = 'http://localhost:5000'
SCRAPER_API_URL = 'http://localhost:8000'
SCREENSHOT_API_URL = 'http://localhost:5002'

# OpenAI Configuration
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-openai-api-key-here')
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-5-nano-2025-08-07')  # GPT-5 Nano with Responses API
MAX_TOKENS = int(os.getenv('MAX_TOKENS', '10000'))

# Initialize OpenAI client
if OPENAI_API_KEY and OPENAI_API_KEY != 'your-openai-api-key-here':
    openai.api_key = OPENAI_API_KEY
    OPENAI_AVAILABLE = True
    print(f"‚úÖ OpenAI configured with model: {OPENAI_MODEL}")
else:
    OPENAI_AVAILABLE = False
    print("‚ö†Ô∏è  OpenAI API key not configured. Using enhanced mock LLM.txt generation.")

# Global storage for processing jobs
processing_jobs = {}

# Global storage for service processes
service_processes = {}

def is_port_in_use(port):
    """Check if a port is already in use (cross-platform)"""
    import socket
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.bind(('localhost', port))
            return False
        except OSError:
            return True

def start_service(service_name, script_path, port, working_dir=None):
    """Start a service in the background (cross-platform)"""
    try:
        if is_port_in_use(port):
            print(f"‚úÖ {service_name} already running on port {port}")
            return True
        
        # Determine the correct Python command for the platform
        python_cmd = "python" if platform.system() == "Windows" else "python3"
        
        # Start the service
        if working_dir:
            process = subprocess.Popen(
                [python_cmd, script_path],
                cwd=working_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                creationflags=subprocess.CREATE_NEW_PROCESS_GROUP if platform.system() == "Windows" else 0
            )
        else:
            process = subprocess.Popen(
                [python_cmd, script_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                creationflags=subprocess.CREATE_NEW_PROCESS_GROUP if platform.system() == "Windows" else 0
            )
        
        # Check if process started successfully
        time.sleep(2)  # Give it a moment to start
        if process.poll() is not None:
            # Process has already terminated
            stdout, stderr = process.communicate()
            print(f"‚ùå {service_name} failed to start:")
            if stderr:
                print(f"   Error: {stderr.decode()}")
            if stdout:
                print(f"   Output: {stdout.decode()}")
            return False
        
        service_processes[service_name] = process
        print(f"üöÄ Started {service_name} with PID: {process.pid}")
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to start {service_name}: {str(e)}")
        return False

def check_service_health(service_name, port, health_endpoint="/health"):
    """Check if a service is healthy"""
    try:
        response = requests.get(f"http://localhost:{port}{health_endpoint}", timeout=5)
        return response.status_code == 200
    except:
        return False

def check_chrome_availability():
    """Check if Chrome/Chromium is available for screenshot service"""
    try:
        # Try to find Chrome/Chromium
        chrome_paths = [
            '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome',  # macOS
            '/usr/bin/google-chrome',  # Linux
            '/usr/bin/chromium-browser',  # Linux
            'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe',  # Windows
            'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe'  # Windows
        ]
        
        for path in chrome_paths:
            if os.path.exists(path):
                print(f"‚úÖ Chrome found at: {path}")
                return True
        
        # Try to run chrome command
        result = subprocess.run(['which', 'google-chrome'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"‚úÖ Chrome found in PATH: {result.stdout.strip()}")
            return True
            
        result = subprocess.run(['which', 'chromium-browser'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"‚úÖ Chromium found in PATH: {result.stdout.strip()}")
            return True
        
        print("‚ö†Ô∏è  Chrome/Chromium not found. Screenshot service may not work properly.")
        print("   Please install Chrome or Chromium for full functionality.")
        return False
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not check Chrome availability: {str(e)}")
        return False

def start_all_services():
    """Start all required services"""
    print("üöÄ Starting required services...")
    print("=" * 40)
    
    # Check Chrome availability
    check_chrome_availability()
    print("")
    
    # Get the base directory
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Service configurations
    services = [
        {
            'name': 'Sitemap Service',
            'script': 'sitemap_api.py',
            'port': 5000,
            'working_dir': os.path.join(base_dir, 'Sitemap_Service')
        },
        {
            'name': 'Scraper Service',
            'script': 'scraper_api.py',
            'port': 8000,
            'working_dir': os.path.join(base_dir, 'Text_Scrapper_Service')
        },
        {
            'name': 'Screenshot Service',
            'script': 'screenshot_api.py',
            'port': 5002,
            'working_dir': os.path.join(base_dir, 'Screenshot_Service')
        }
    ]
    
    # Start each service
    for service in services:
        script_path = os.path.join(service['working_dir'], service['script'])
        if os.path.exists(script_path):
            start_service(service['name'], service['script'], service['port'], service['working_dir'])
        else:
            print(f"‚ùå {service['name']} script not found: {script_path}")
    
    # Wait for services to initialize (longer on Windows due to socket issues)
    print("‚è≥ Waiting for services to initialize...")
    wait_time = 8 if platform.system() == "Windows" else 5
    time.sleep(wait_time)
    
    # Check service health
    print("üîç Checking service health...")
    all_healthy = True
    
    for service in services:
        if check_service_health(service['name'], service['port']):
            print(f"‚úÖ {service['name']}: Healthy")
        else:
            print(f"‚ùå {service['name']}: Not responding")
            all_healthy = False
    
    if all_healthy:
        print("üéØ All services are ready!")
    else:
        print("‚ö†Ô∏è  Some services may not be ready. The app will still work with fallbacks.")
    
    print("=" * 40)

def cleanup_services():
    """Clean up all service processes"""
    print("üõë Stopping all services...")
    
    for service_name, process in service_processes.items():
        try:
            if platform.system() == "Windows":
                process.terminate()
            else:
                process.terminate()
                process.wait(timeout=5)
            print(f"‚úÖ Stopped {service_name}")
        except:
            try:
                process.kill()
                print(f"üî® Force killed {service_name}")
            except:
                print(f"‚ùå Failed to stop {service_name}")
    
    service_processes.clear()

def signal_handler(signum, frame):
    """Handle shutdown signals"""
    print("\nüõë Shutdown signal received...")
    cleanup_services()
    sys.exit(0)

# Register signal handlers for graceful shutdown
if platform.system() != "Windows":
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

class LLMGeneratorJob:
    """Class to manage LLM.txt generation jobs"""
    
    def __init__(self, job_id: str, url: str):
        self.job_id = job_id
        self.url = url
        self.status = "initializing"
        self.progress = 0
        self.logs = []
        self.start_time = datetime.now()
        self.end_time = None
        self.error = None
        self.results = []
        
    def log(self, message: str, level: str = "info"):
        """Add a log entry with timestamp and emit to frontend"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = {
            'timestamp': timestamp,
            'message': message,
            'level': level
        }
        self.logs.append(log_entry)
        
        # Emit log to frontend via SocketIO
        socketio.emit('job_log', {
            'job_id': self.job_id,
            'log': log_entry
        }, room=self.job_id)
        
        print(f"[{timestamp}] [{level.upper()}] {message}")
    
    def update_status(self, status: str, progress: int = None):
        """Update job status and progress"""
        self.status = status
        if progress is not None:
            self.progress = progress
            
        # Emit status update to frontend
        socketio.emit('job_status', {
            'job_id': self.job_id,
            'status': self.status,
            'progress': self.progress
        }, room=self.job_id)
        
        self.log(f"Status: {status} ({self.progress}%)")
    
    def set_error(self, error_message: str):
        """Set error status and message"""
        self.error = error_message
        self.status = "error"
        self.end_time = datetime.now()
        self.log(f"Error: {error_message}", "error")
        
        # Emit error to frontend
        socketio.emit('job_error', {
            'job_id': self.job_id,
            'error': error_message
        }, room=self.job_id)
    
    def complete(self):
        """Mark job as completed"""
        self.status = "completed"
        self.progress = 100
        self.end_time = datetime.now()
        duration = (self.end_time - self.start_time).total_seconds()
        self.log(f"Job completed in {duration:.2f} seconds", "success")
        
        # Emit completion to frontend
        socketio.emit('job_complete', {
            'job_id': self.job_id,
            'results': self.results
        }, room=self.job_id)

def init_database():
    """Initialize SQLite database for storing results"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    # Create tables
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS llm_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id TEXT NOT NULL,
            url TEXT NOT NULL,
            llm_txt TEXT NOT NULL,
            screenshot_path TEXT,
            scraped_text_count INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS jobs (
            id TEXT PRIMARY KEY,
            original_url TEXT NOT NULL,
            status TEXT NOT NULL,
            progress INTEGER DEFAULT 0,
            start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            end_time TIMESTAMP,
            error_message TEXT,
            total_pages INTEGER DEFAULT 0
        )
    ''')
    
    conn.commit()
    conn.close()

def save_job_to_db(job: LLMGeneratorJob):
    """Save job information to database"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    cursor.execute('''
        INSERT OR REPLACE INTO jobs 
        (id, original_url, status, progress, start_time, end_time, error_message, total_pages)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        job.job_id,
        job.url,
        job.status,
        job.progress,
        job.start_time,
        job.end_time,
        job.error,
        len(job.results)
    ))
    
    conn.commit()
    conn.close()

def save_result_to_db(job_id: str, url: str, llm_txt: str, screenshot_path: str = None, scraped_text_count: int = 0):
    """Save individual result to database"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    cursor.execute('''
        INSERT INTO llm_results 
        (job_id, url, llm_txt, screenshot_path, scraped_text_count)
        VALUES (?, ?, ?, ?, ?)
    ''', (job_id, url, llm_txt, screenshot_path, scraped_text_count))
    
    conn.commit()
    conn.close()

def get_sitemap(job: LLMGeneratorJob) -> Optional[Dict]:
    """Get sitemap from the sitemap API"""
    job.log("Starting sitemap generation...")
    job.update_status("generating_sitemap", 10)
    
    try:
        # Start sitemap generation
        response = requests.post(f"{SITEMAP_API_URL}/generate-sitemap", 
                               json={'url': job.url, 'max_depth': 3}, 
                               timeout=10)
        
        if response.status_code != 202:
            raise Exception(f"Failed to start sitemap generation: {response.status_code}")
        
        sitemap_job_data = response.json()
        sitemap_job_id = sitemap_job_data['job_id']
        
        job.log(f"Sitemap job started with ID: {sitemap_job_id}")
        
        # Poll for completion
        max_attempts = 120  # 10 minutes max (increased from 5 minutes)
        for attempt in range(max_attempts):
            time.sleep(5)  # Wait 5 seconds between checks
            
            status_response = requests.get(f"{SITEMAP_API_URL}/status/{sitemap_job_id}", timeout=10)
            if status_response.status_code != 200:
                continue
                
            status_data = status_response.json()
            
            if status_data['status'] == 'completed':
                job.log(f"Sitemap generated successfully! Found {status_data['discovered_urls_count']} URLs")
                return status_data['sitemap']
            elif status_data['status'] == 'error':
                raise Exception(f"Sitemap generation failed: {status_data.get('error', 'Unknown error')}")
            else:
                job.log(f"Sitemap generation in progress... ({status_data['status']})")
        
        raise Exception("Sitemap generation timed out")
        
    except Exception as e:
        job.set_error(f"Sitemap generation failed: {str(e)}")
        return None

def extract_urls_from_sitemap(sitemap: Dict) -> List[str]:
    """Extract all URLs from sitemap structure"""
    urls = []
    
    def extract_recursive(data, path=""):
        if isinstance(data, dict):
            for key, value in data.items():
                current_path = f"{path}.{key}" if path else key
                
                # Check if the key itself is a URL (with or without protocol)
                if isinstance(key, str):
                    if key.startswith('http'):
                        urls.append(key)
                    elif '.' in key and not key.startswith('http'):  # Domain name without protocol
                        url_with_protocol = f"https://{key}"
                        urls.append(url_with_protocol)
                
                # Check if the value is a URL (with or without protocol)
                if isinstance(value, str):
                    if value.startswith('http'):
                        urls.append(value)
                    elif '.' in value and not value.startswith('http'):  # Domain name without protocol
                        url_with_protocol = f"https://{value}"
                        urls.append(url_with_protocol)
                
                # Recursively process nested data
                extract_recursive(value, current_path)
                
        elif isinstance(data, list):
            for i, item in enumerate(data):
                current_path = f"{path}[{i}]"
                
                if isinstance(item, str):
                    if item.startswith('http'):
                        urls.append(item)
                    elif '.' in item and not item.startswith('http'):  # Domain name without protocol
                        url_with_protocol = f"https://{item}"
                        urls.append(url_with_protocol)
                else:
                    extract_recursive(item, current_path)
    
    extract_recursive(sitemap)
    unique_urls = list(set(urls))  # Remove duplicates
    return unique_urls

def scrape_text(job: LLMGeneratorJob, url: str) -> Optional[List[str]]:
    """Scrape text content from a URL using the scraper API"""
    try:
        # Start scraping job
        response = requests.post(f"{SCRAPER_API_URL}/scrape", 
                               json={'url': url}, 
                               timeout=10)
        
        if response.status_code != 202:
            raise Exception(f"Failed to start scraping: {response.status_code}")
        
        scrape_job_data = response.json()
        scrape_job_id = scrape_job_data['job_id']
        
        # Poll for completion
        max_attempts = 12  # 1 minute max (12 √ó 5 seconds)
        for attempt in range(max_attempts):
            time.sleep(5)  # Wait 5 seconds between checks
            
            status_response = requests.get(f"{SCRAPER_API_URL}/jobs/{scrape_job_id}", timeout=10)
            if status_response.status_code != 200:
                continue
                
            status_data = status_response.json()
            
            if status_data['status'] == 'completed':
                return status_data['result']['text_content']
            elif status_data['status'] == 'failed':
                raise Exception(f"Scraping failed: {status_data['result'].get('error', 'Unknown error')}")
        
        raise Exception("Scraping timed out")
        
    except Exception as e:
        job.log(f"Failed to scrape {url}: {str(e)}", "warning")
        return None

def take_screenshot(job: LLMGeneratorJob, url: str) -> Optional[str]:
    """Take screenshot of a URL using the screenshot API"""
    try:
        response = requests.post(f"{SCREENSHOT_API_URL}/screenshot", 
                               json={'url': url}, 
                               timeout=60)
        
        if response.status_code != 200:
            raise Exception(f"Screenshot failed: {response.status_code}")
        
        screenshot_data = response.json()
        
        if screenshot_data.get('success'):
            return screenshot_data.get('file_path')
        else:
            raise Exception(screenshot_data.get('error', 'Unknown error'))
            
    except Exception as e:
        job.log(f"Failed to screenshot {url}: {str(e)}", "warning")
        return None

def generate_llm_txt(job: LLMGeneratorJob, url: str, text_content: List[str], screenshot_path: str = None) -> str:
    """Generate LLM.txt content using ChatGPT API"""
    try:
        # Prepare the content for ChatGPT
        text_summary = "\n".join(text_content[:50])  # Limit to first 50 items to avoid token limits
        
        # Create comprehensive prompt for LLM.txt generation with multimodal analysis
        prompt = f"""You are an expert AI content analyst tasked with creating a comprehensive LLM.txt file for a webpage. LLM.txt files are designed to provide AI systems with structured, detailed information about webpage content for better understanding, SEO optimization, and AI training.

**TARGET URL**: {url}

**EXTRACTED TEXT CONTENT**:
{text_summary}

**VISUAL ANALYSIS**: {"A screenshot of the webpage is provided for visual analysis" if screenshot_path else "No visual content available"}

**YOUR TASK**: Create a comprehensive LLM.txt file that includes:

1. **Page Information**: URL, title, generation timestamp
2. **Content Analysis**: Detailed summary of what the page contains (combine text and visual analysis)
3. **Key Topics & Themes**: Main subjects and themes covered
4. **Keywords & Entities**: Important keywords, names, places, organizations
5. **Page Purpose**: Primary goal and intended use of this page
6. **Content Structure**: How information is organized (sections, categories, etc.)
7. **Visual Elements**: {"Describe layout, design, images, and visual hierarchy from the screenshot" if screenshot_path else "Visual analysis not available"}
8. **Target Audience**: Who this content is designed for
9. **SEO Elements**: Meta information, headings, call-to-actions
10. **Technical Details**: Content type, structure, accessibility features
11. **AI-Friendly Summary**: Optimized summary for AI systems and search engines

**ANALYSIS INSTRUCTIONS**:
- Analyze both the extracted text content and the visual screenshot (if provided)
- Identify visual design patterns, color schemes, and layout structure
- Note any images, graphics, or visual elements that enhance the content
- Consider the user experience and visual hierarchy
- Combine textual and visual insights for comprehensive analysis

**FORMATTING REQUIREMENTS**:
- Use clear markdown formatting with headers (##)
- Include bullet points for lists
- Be comprehensive but concise
- Focus on SEO and AI optimization
- Make it useful for both humans and AI systems
- Include visual analysis section if screenshot is available

**OUTPUT**: Return ONLY the LLM.txt content, no additional text or explanations."""

        # Use real ChatGPT API if available
        if OPENAI_AVAILABLE:
            job.log(f"Generating LLM.txt using {OPENAI_MODEL} with Responses API...")
            
            try:
                # Prepare the request for GPT-5 Nano with Responses API
                # This API supports both text and image analysis
                
                # Create the request payload for Responses API
                request_data = {
                    "model": OPENAI_MODEL,
                    "messages": [
                        {
                            "role": "system", 
                            "content": "You are an expert AI content analyst specializing in creating comprehensive LLM.txt files for web pages. You excel at analyzing content structure, identifying key themes, and creating SEO-optimized summaries that are valuable for both AI systems and search engines. You can analyze both text content and visual elements from screenshots."
                        },
                        {
                            "role": "user", 
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                }
                            ]
                        }
                    ],
                    "max_tokens": MAX_TOKENS,
                    "temperature": 0.3,  # Lower temperature for more consistent, factual output
                    "top_p": 0.9
                }
                
                # Add image analysis if screenshot is available
                if screenshot_path and os.path.exists(screenshot_path):
                    job.log(f"Including screenshot analysis: {screenshot_path}")
                    try:
                        # Read and encode the screenshot
                        import base64
                        with open(screenshot_path, "rb") as image_file:
                            image_data = base64.b64encode(image_file.read()).decode('utf-8')
                        
                        # Add image to the request
                        request_data["messages"][1]["content"].append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_data}",
                                "detail": "high"  # High detail for better analysis
                            }
                        })
                        
                        job.log("Screenshot added for visual analysis")
                        
                    except Exception as img_error:
                        job.log(f"Failed to include screenshot: {str(img_error)}", "warning")
                
                # Make the API call using the new Responses API format
                # GPT-5 Nano only supports default temperature (1), so we remove custom parameters
                response = openai.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=request_data["messages"],
                    max_completion_tokens=request_data["max_tokens"]  # Use max_completion_tokens for GPT-5
                    # Note: temperature and top_p are not supported by GPT-5 Nano
                )
                
                llm_txt_content = response.choices[0].message.content.strip()
                job.log(f"Successfully generated LLM.txt using {OPENAI_MODEL} with multimodal analysis")
                job.log(f"Generated LLM.txt content length: {len(llm_txt_content)} characters")
                return llm_txt_content
                
            except Exception as api_error:
                job.log(f"OpenAI API error: {str(api_error)}", "warning")
                job.log("Falling back to enhanced mock generation...")
                # Fall through to enhanced mock generation
        
        # Enhanced mock generation (fallback or when OpenAI is not available)
        job.log("Using enhanced mock LLM.txt generation...")
        
        # Extract key information from text content
        page_title = text_content[0] if text_content else url.split('/')[-1]
        
        # Enhanced mock content with better structure
        llm_txt_content = f"""# LLM.txt for {url}

## Page Information
- **URL**: {url}
- **Title**: {page_title}
- **Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Content Analysis Method**: {"AI-Powered (ChatGPT)" if OPENAI_AVAILABLE else "Enhanced Template-Based"}

## Content Summary
This webpage contains {len(text_content)} text elements providing comprehensive information across multiple topics. The content appears to be well-structured and designed for web consumption.

## Key Content Elements
{chr(10).join([f"- {item}" for item in text_content[:10]])}

## Page Structure
- **Total Text Elements**: {len(text_content)}
- **Content Type**: {"Website page" if "http" in url else "Web resource"}
- **Primary Focus**: Web content analysis and information delivery
- **Structure**: Hierarchical content organization

## Keywords and Topics
Based on the extracted content, this page covers topics related to:
{', '.join(set([word.lower() for item in text_content[:5] for word in item.split() if len(word) > 3][:10]))}

## Target Audience
- Web users seeking information
- Search engine crawlers
- AI systems for content analysis
- General internet audience

## SEO Elements
- **Meta Information**: Available through content analysis
- **Content Structure**: Well-organized with clear hierarchy
- **Keywords**: Extracted from content analysis
- **Call-to-Actions**: Present in content structure

## Technical Details
- **Screenshot Available**: {"Yes" if screenshot_path else "No"}
- **Content Extraction Method**: Automated web scraping with Selenium
- **Processing Timestamp**: {datetime.now().isoformat()}
- **Content Format**: HTML-based web content
- **Accessibility**: Standard web accessibility features

## AI-Friendly Summary
This webpage provides structured information and content that can be effectively categorized and understood by AI systems. The content is optimized for search engine discovery and includes comprehensive information suitable for automated analysis, content indexing, and AI training purposes. The page serves as a valuable resource for both human users and artificial intelligence systems seeking to understand web content structure and information delivery.

## Content Quality Assessment
- **Completeness**: Comprehensive content coverage
- **Clarity**: Well-structured information presentation
- **Relevance**: Content appears relevant to page purpose
- **Usability**: Suitable for both human and AI consumption
"""
        
        job.log(f"Generated mock LLM.txt content length: {len(llm_txt_content)} characters")
        return llm_txt_content
        
    except Exception as e:
        job.log(f"Failed to generate LLM.txt for {url}: {str(e)}", "error")
        return f"""# LLM.txt Generation Failed for {url}

## Error Information
- **Error**: {str(e)}
- **Generated**: {datetime.now().isoformat()}
- **Status**: Generation failed due to technical error

## Fallback Content
This page was processed but LLM.txt generation encountered an error. The page URL is {url} and was processed as part of a website analysis workflow.

## Technical Details
- **Processing Method**: Automated web scraping
- **Error Type**: LLM.txt generation failure
- **Timestamp**: {datetime.now().isoformat()}
"""

def process_website(job_id: str):
    """Main processing function that orchestrates the entire workflow step-by-step"""
    job = processing_jobs[job_id]
    
    try:
        # Step 1: Generate sitemap
        job.log("Step 1: Generating sitemap...")
        sitemap = get_sitemap(job)
        if not sitemap:
            return
        
        # Step 2: Extract URLs from sitemap
        job.log("Step 2: Extracting URLs from sitemap...")
        urls = extract_urls_from_sitemap(sitemap)
        job.log(f"Extracted {len(urls)} URLs from sitemap")
        job.update_status("extracting_urls", 15)
        
        if not urls:
            job.log("No URLs found in sitemap", "warning")
            job.complete()
            return
        
        # Step 3: Scrape text content for all URLs (parallel processing)
        job.log("Step 3: Scraping text content for all pages...")
        job.update_status("scraping_text", 25)
        
        text_contents = {}
        for i, url in enumerate(urls):
            job.log(f"Scraping text {i+1}/{len(urls)}: {url}")
            text_content = scrape_text(job, url)
            if text_content:
                text_contents[url] = text_content
                job.log(f"Successfully scraped {len(text_content)} text elements from {url}")
            else:
                job.log(f"Failed to scrape text from {url}", "warning")
                text_contents[url] = [f"Failed to extract content from {url}"]
            
            progress = 25 + int((i + 1) / len(urls) * 25)  # 25-50%
            job.update_status("scraping_text", progress)
        
        # Step 4: Take screenshots for all URLs (parallel processing)
        job.log("Step 4: Taking screenshots for all pages...")
        job.update_status("taking_screenshots", 50)
        
        screenshot_paths = {}
        for i, url in enumerate(urls):
            job.log(f"Taking screenshot {i+1}/{len(urls)}: {url}")
            screenshot_path = take_screenshot(job, url)
            if screenshot_path:
                screenshot_paths[url] = screenshot_path
                job.log(f"Successfully captured screenshot for {url}")
            else:
                job.log(f"Failed to take screenshot of {url}", "warning")
                screenshot_paths[url] = None
            
            progress = 50 + int((i + 1) / len(urls) * 25)  # 50-75%
            job.update_status("taking_screenshots", progress)
        
        # Step 5: Generate LLM.txt for all URLs (parallel processing)
        job.log("Step 5: Generating LLM.txt files for all pages...")
        job.update_status("generating_llm", 75)
        
        for i, url in enumerate(urls):
            job.log(f"Generating LLM.txt {i+1}/{len(urls)}: {url}")
            
            text_content = text_contents.get(url, [])
            screenshot_path = screenshot_paths.get(url)
            
            # Generate LLM.txt
            llm_txt = generate_llm_txt(job, url, text_content, screenshot_path)
            
            # Save to database
            save_result_to_db(job.job_id, url, llm_txt, screenshot_path, len(text_content))
            
            # Add to job results
            job.results.append({
                'url': url,
                'llm_txt': llm_txt,
                'screenshot_path': screenshot_path,
                'text_count': len(text_content)
            })
            
            progress = 75 + int((i + 1) / len(urls) * 20)  # 75-95%
            job.update_status("generating_llm", progress)
            
            job.log(f"Successfully generated LLM.txt for {url}")
        
        # Step 6: Complete job
        job.log("Step 6: Finalizing results...")
        job.update_status("finalizing", 95)
        save_job_to_db(job)
        job.complete()
        
        job.log(f"Job completed successfully! Processed {len(urls)} pages.")
        
    except Exception as e:
        job.log(f"Job failed with error: {str(e)}", "error")
        job.set_error(str(e))
        save_job_to_db(job)

# Flask Routes
@app.route('/')
def index():
    """Main page route"""
    return render_template('index.html')

@app.route('/api/start-job', methods=['POST'])
def start_job():
    """Start a new LLM.txt generation job"""
    try:
        data = request.get_json()
        url = data.get('url', '').strip()
        
        if not url:
            return jsonify({'error': 'URL is required'}), 400
        
        # Validate URL format
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # Create new job
        job_id = str(uuid.uuid4())
        job = LLMGeneratorJob(job_id, url)
        processing_jobs[job_id] = job
        
        # Save initial job to database
        save_job_to_db(job)
        
        # Start processing in background thread
        thread = threading.Thread(target=process_website, args=(job_id,))
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'job_id': job_id,
            'status': 'started',
            'message': 'LLM.txt generation started'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/job-status/<job_id>')
def get_job_status(job_id):
    """Get job status and results"""
    try:
        if job_id not in processing_jobs:
            return jsonify({'error': 'Job not found'}), 404
        
        job = processing_jobs[job_id]
        
        return jsonify({
            'job_id': job_id,
            'status': job.status,
            'progress': job.progress,
            'url': job.url,
            'logs': job.logs,
            'results': job.results,
            'error': job.error,
            'start_time': job.start_time.isoformat() if job.start_time else None,
            'end_time': job.end_time.isoformat() if job.end_time else None
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/export-results/<job_id>')
def export_results(job_id):
    """Export results as JSON file"""
    try:
        if job_id not in processing_jobs:
            return jsonify({'error': 'Job not found'}), 404
        
        job = processing_jobs[job_id]
        
        # Create export data
        export_data = {
            'job_info': {
                'job_id': job_id,
                'original_url': job.url,
                'status': job.status,
                'total_pages': len(job.results),
                'generated_at': datetime.now().isoformat()
            },
            'results': job.results
        }
        
        # Save to temporary file
        export_filename = f"llm_results_{job_id}.json"
        export_path = f"/tmp/{export_filename}"
        
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        return send_file(export_path, as_attachment=True, download_name=export_filename)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# SocketIO Events
@socketio.on('connect')
def handle_connect():
    """Handle client connection"""
    print('Client connected')

@socketio.on('disconnect')
def handle_disconnect():
    """Handle client disconnection"""
    print('Client disconnected')

@socketio.on('join_job')
def handle_join_job(data):
    """Join a specific job room for real-time updates"""
    job_id = data.get('job_id')
    if job_id:
        from flask_socketio import join_room
        join_room(job_id)
        emit('joined_job', {'job_id': job_id})

if __name__ == '__main__':
    try:
        # Initialize database
        init_database()
        
        print("üöÄ Starting LLM.txt Generator Web Application")
        print("=" * 50)
        print("üìã Features:")
        print("  - Professional UI/UX with real-time updates")
        print("  - Sitemap generation and URL discovery")
        print("  - Text content extraction")
        print("  - Screenshot capture")
        print("  - LLM.txt generation")
        print("  - Database storage and export")
        print("  - Comprehensive logging and error handling")
        print("=" * 50)
        
        # Start all required services automatically
        start_all_services()
        
        print("üåê Web Interface: http://localhost:5001")
        print("üìä Database: SQLite (llm_generator.db)")
        print("=" * 50)
        print("üí° The application is ready to use!")
        print("   Just open your browser and go to http://localhost:5001")
        print("   Press Ctrl+C to stop the application and all services")
        print("=" * 50)
        
        # Run the application
        # Disable debug mode on Windows to avoid socket conflicts
        debug_mode = platform.system() != 'Windows'
        socketio.run(app, host='0.0.0.0', port=5001, debug=debug_mode)
        
    except KeyboardInterrupt:
        print("\nüõë Application stopped by user")
        cleanup_services()
    except Exception as e:
        print(f"\n‚ùå Application error: {str(e)}")
        cleanup_services()
        sys.exit(1)
